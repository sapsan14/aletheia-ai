# Agent Audit Model

**Provable, reviewable, and explainable behavior tracking for autonomous and semi-autonomous LLM agents.**

---

## Table of Contents

- [Purpose](#purpose)
- [What is an Agent Action](#what-is-an-agent-action)
- [Audit Philosophy](#audit-philosophy)
- [Audit Record Structure](#audit-record-structure)
- [What Gets Signed](#what-gets-signed)
- [Cryptographic Guarantees](#cryptographic-guarantees)
- [Responsibility Boundary](#responsibility-boundary)
- [Human-in-the-Loop](#human-in-the-loop)
- [Multi-Agent Systems](#multi-agent-systems)
- [Why This Matters](#why-this-matters)
- [Example Incident Explanation](#example-incident-explanation)
- [Future Extensions](#future-extensions)
- [Summary](#summary)
- [Related Documents](#related-documents)

---

## Purpose

This document describes the **audit model for autonomous or semi-autonomous LLM agents**.

Its goal is to make agent behavior **provable, reviewable, and explainable after the fact**.

**In short:**

> Not "why the AI thought so",  
> but **what exactly happened, when, and under which conditions**.

This model is designed to work with cryptographic signing and timestamping as implemented in the Aletheia AI PoC.

---

## What is an Agent Action

An **agent action** is any output that may influence the real world.

**Examples:**

- Textual recommendation
- Decision suggestion
- Instruction generated for another system
- API call proposal
- Tool execution plan
- Message sent to user or another agent

**Important:**

Even if the agent does *not* execute the action itself, **the suggestion alone may carry responsibility**.

Example: An agent recommends a medical dosage. Even if a human must approve it, the recommendation itself is an action with potential consequences.

---

## Audit Philosophy

The system does **not** attempt to prove that the answer is correct.

Instead, it proves:

| Aspect | What is Proven |
|--------|----------------|
| **Content** | What the agent produced |
| **Time** | When it was produced |
| **Configuration** | Under which configuration (temperature, top_p, etc.) |
| **Model** | By which model and version |
| **Integrity** | That it was not altered later |

**This mirrors principles used in:**

- Financial systems (transaction logs, audit trails)
- PKI (certificate transparency, signature timestamps)
- Incident response (forensic evidence)
- Black box flight recorders (immutable event logs)

**Goal:** Create an **immutable record of what happened**, not a judgment of whether it was "right."

---

## Audit Record Structure

Each agent action generates an **immutable audit record**.

### Minimum Fields

```json
{
  "agent_id": "logistics-agent-v1",
  "request_id": "uuid",
  "timestamp": "2026-01-31T16:15:00Z",
  "model": "gpt-4.1",
  "model_version": "gpt-4.1-turbo-2026-01-25",
  "prompt": "Calculate optimal route for delivery...",
  "context": "User location: Berlin, Destination: Munich, constraints: ...",
  "response": "Recommended route: A9 via Nuremberg. ETA: 4.5 hours.",
  "parameters": {
    "temperature": 0.2,
    "top_p": 0.9,
    "max_tokens": 500
  }
}
```

This structure is called the **Agent Evidence Payload**.

### Field Descriptions

| Field | Description | Required |
|-------|-------------|----------|
| `agent_id` | Unique identifier for the agent instance | ✅ Yes |
| `request_id` | Unique ID for this request (for correlation) | ✅ Yes |
| `timestamp` | UTC timestamp when response was generated | ✅ Yes |
| `model` | Model name (e.g., "gpt-4.1", "gemini-pro") | ✅ Yes |
| `model_version` | Exact version/snapshot identifier | ✅ Yes |
| `prompt` | Input prompt sent to the model | ✅ Yes |
| `context` | Additional context (conversation history, system prompt, etc.) | Recommended |
| `response` | Output generated by the agent | ✅ Yes |
| `parameters` | Model parameters (temperature, top_p, seed, etc.) | ✅ Yes |
| `tools_used` | List of tools/APIs called (if applicable) | Optional |
| `metadata` | Additional metadata (user_id, session_id, etc.) | Optional |

---

## What Gets Signed

The system creates a **canonical representation** of the audit payload.

### Process

```
1. Canonicalize payload (deterministic JSON/text representation)
2. hash = SHA-256(canonical_payload)
3. signature = Sign(hash, agent_private_key)
4. timestamp = RFC3161(signature)
```

**Recommended approach (from [Aletheia pipeline](../../diagrams/architecture.md)):**

```
canonical_payload → hash → sign → timestamp(signature)
```

This ensures that both **content** and **responsibility** are frozen in time.

### Why Timestamp the Signature?

- **Content integrity:** The hash proves the payload hasn't changed.
- **Time proof:** The timestamp proves the signature existed at time T.
- **Chain of trust:** TSA attests to the *when*, agent key attests to the *what*.

See [Trust Model](TRUST_MODEL.md) for details on who attests what.

---

## Cryptographic Guarantees

The audit record allows anyone to verify:

| Guarantee | What It Proves |
|-----------|----------------|
| ✅ **Content integrity** | The content has not changed since signing |
| ✅ **Authentication** | The signature belongs to the agent/system (via public key) |
| ✅ **Non-repudiation** | The agent cannot deny generating this output |
| ✅ **Time proof** | The timestamp proves existence at time T (via TSA) |
| ✅ **Chronological order** | The record predates any incident or dispute |

**This creates non-repudiation for AI behavior.**

Anyone with:
- The audit record (payload + signature + timestamp)
- The agent's public key
- The TSA's public key/certificate

can independently verify all guarantees.

---

## Responsibility Boundary

**Important clarification:**

| What Signature Means | What It Does NOT Mean |
|----------------------|------------------------|
| ✅ This exact output was generated by this agent | ❌ The answer is correct |
| ✅ At this specific time | ❌ The agent is legally responsible |
| ✅ Under this configuration | ❌ The output is safe or appropriate |
| ✅ Content is unchanged | ❌ The decision should be followed |

**The signature proves:**

> "Who generated and presented the output."

**It does NOT prove:**

> "That the output is correct or should be trusted."

### Analogy

This is similar to:

- **Dashcam recordings** — prove what happened, not who is at fault
- **System logs** — record events, not correctness
- **Email DKIM signatures** — prove sender, not content validity
- **Notarization** — attests to signature, not document truth

### Example Statement

> "At time T, our system received this exact output from model M and provided it unchanged to the user. We do not attest to the correctness of the output, but we can prove this is what was generated."

---

## Human-in-the-Loop

If a human approves or modifies the output, this becomes a **new audit event**.

### Example Flow

```
1. Agent generates output → Audit Record A (signed + timestamped)
   ↓
2. Human reviews and modifies → Audit Record B (signed + timestamped)
   ↓
3. Modified response sent to user → linked to both A and B
```

### Audit Record for Human Action

```json
{
  "action_type": "human_review",
  "reviewer_id": "user@example.com",
  "timestamp": "2026-01-31T16:20:00Z",
  "original_response_id": "uuid-from-record-A",
  "modified_response": "Updated recommendation based on additional constraints...",
  "changes": "Added constraint X, removed suggestion Y",
  "approval_status": "approved_with_modifications"
}
```

**Each step must produce a separate audit record.**

This preserves full traceability:
- What did the agent originally say?
- What did the human change?
- When did each action occur?

---

## Multi-Agent Systems

For agent chains (MCP, tool agents, planners, multi-agent workflows):

**Each agent must produce its own audit record.**

### Example Chain

```
Planner Agent → Audit Record 1
   ↓
Tool Agent → Audit Record 2
   ↓
Execution Agent → Audit Record 3
```

### Chain Structure

```json
{
  "agent_id": "planner-agent",
  "request_id": "chain-uuid",
  "response": "Plan: Step 1...",
  "next_agent": "tool-agent",
  "chain_position": 1
}
```

```json
{
  "agent_id": "tool-agent",
  "request_id": "chain-uuid",
  "previous_agent": "planner-agent",
  "response": "Tool execution result...",
  "chain_position": 2
}
```

### Benefits

- **Reconstruction:** Full reasoning timeline can be reconstructed
- **Blame isolation:** Identify which agent in the chain caused an issue
- **Privacy:** Internal chain-of-thought is not exposed (only final outputs)
- **Granular audit:** Each step is independently verifiable

### Cross-Agent References

Use `chain_id` or `parent_request_id` to link related audit records across agents.

---

## Why This Matters

This model becomes **critical** when:

| Scenario | Risk Without Audit Model |
|----------|--------------------------|
| **Agents operate continuously** | No record of decisions made overnight |
| **Agents control tools** | Cannot prove what actions were actually taken |
| **Agents affect money, logistics, safety** | Liability unclear in case of error |
| **Agents give advice to vulnerable users** | Cannot verify what was actually said |
| **Regulators ask "who said this and when?"** | No forensic evidence available |

### Especially Relevant Cases

- **Logistics errors** — wrong route, delayed delivery, damaged goods
- **Automated decisioning** — credit approval, insurance claims, hiring
- **Financial recommendations** — investment advice, trading decisions
- **Content moderation** — false positives/negatives, censorship disputes
- **Child safety** — inappropriate content shown to minors
- **Autonomous agent marketplaces** — agent A calls agent B, who is responsible?

### Real-World Scenario

**Question:** "Why did the agent recommend this risky investment?"

**Without audit model:** "We don't know, the logs were overwritten."

**With audit model:** "Here is the exact prompt, model version, temperature, and output, cryptographically sealed at timestamp T. The recommendation was based on context X."

---

## Example Incident Explanation

**Scenario:** An agent recommends a logistics route that causes a delay.

**Response with Agent Audit Model:**

> "This recommendation was generated by **agent logistics-agent-v1**  
> using model **gpt-4.1-turbo-2026-01-25**  
> at **2026-01-31 16:15:00 UTC**  
> and was cryptographically sealed at that moment via signature and RFC 3161 timestamp.  
>  
> The system did not modify the content afterward.  
>  
> The agent was operating with temperature=0.2, given the following context: [context details].  
>  
> Here is the signed audit record: [link to verification portal]."

**This is the AI equivalent of:**

> "Here is the black box recording."

### Verification Portal

Users, regulators, or auditors can:
1. Download the audit record (JSON)
2. Verify the signature against the agent's public key
3. Verify the timestamp against the TSA's certificate
4. Inspect the exact input/output/configuration

All without needing to trust Aletheia's word.

---

## Future Extensions

Planned or compatible extensions for enhanced auditability:

| Extension | Benefit |
|-----------|---------|
| **eIDAS-qualified TSA** | Legal-grade timestamps (EU regulation) |
| **Qualified certificates** | Agent keys certified by trusted CA |
| **HSM-backed agent keys** | Hardware protection for agent private keys |
| **Per-agent identity certificates** | Each agent has verifiable identity |
| **Public verification portal** | Anyone can verify audit records online |
| **Agent reputation systems** | Track agent performance over time |
| **Audit record compression** | Store millions of records efficiently |
| **Zero-knowledge proofs** | Prove properties without revealing full record |

See [Trust Model (eIDAS mapping)](TRUST_MODEL.md#eidas-mapping-non-qualified--qualified) for upgrade path to qualified trust services.

---

## Summary

**AGENT_AUDIT_MODEL is not about trust.**

It is about:

| Principle | Description |
|-----------|-------------|
| **Memory** | Permanent record of what happened |
| **Accountability** | Clear trail of who did what |
| **Chronology** | Irrefutable timestamps |
| **Evidence** | Cryptographic proof, not just logs |
| **Auditability** | Independent verification by third parties |

### Final Statement

> AI systems will make mistakes.  
>  
> This model ensures that when they do,  
> **the truth of what happened does not disappear.**

---

## Related Documents

- [Signing (SIGNING)](SIGNING.md) — How agent outputs are signed (RSA PKCS#1 v1.5)
- [Timestamping (TIMESTAMPING)](TIMESTAMPING.md) — RFC 3161 timestamps for agent actions
- [Trust Model (TRUST_MODEL)](TRUST_MODEL.md) — Who attests what, eIDAS mapping
- [Cryptographic Oracle (CRYPTO_ORACLE)](CRYPTO_ORACLE.md) — Testing agent outputs for reproducibility
- [MOCK_TSA (MOCK_TSA)](MOCK_TSA.md) — Deterministic TSA for testing agent audit records
- [Architecture Diagrams](../../diagrams/architecture.md) — Pipeline: canonicalize → hash → sign → timestamp
- [Implementation Plan (PLAN)](PLAN.md) — Task roadmap for implementing audit trail
- [README](../../README.md) — Project overview, design philosophy

---

**Status:** Conceptual model. Implementation planned for PoC and beyond.

**License:** MIT (per Aletheia AI project).
